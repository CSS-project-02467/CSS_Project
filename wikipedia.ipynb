{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing the Network of Bots on the Danish Wikipedia\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "For many years now Wikipedia has been one of the most important sources of information on the internet, with millions of articles in all the world's major languages. Relying solely on the collaboration of strangers, Wikipedia has gone from being a radical experiment in free information to a trustworthy source of knowledge. As the project has grown over the years, automation has been vital in keeping articles up to date, making otherwise tedious edits, and empowering contributors to make the most use of what they have to contribute. In this project we take a look at the bots that maintain and expand the Danish Wikipedia, how they work together, and see how network- and textual analysis can shed light on their contributions to the wiki.\n",
    "\n",
    "First, we load the packages, and datasets we've collected."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2f635715bfbf180"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/final/pages_dataframe_final.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 28\u001B[0m\n\u001B[0;32m     25\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(stopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdanish\u001B[39m\u001B[38;5;124m'\u001B[39m)) \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mset\u001B[39m(stopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Load data\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m pages_dataframe \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_pickle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/final/pages_dataframe_final.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     29\u001B[0m bots_dataframe \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_pickle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/final/bots_dataframe_final\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\pickle.py:179\u001B[0m, in \u001B[0;36mread_pickle\u001B[1;34m(filepath_or_buffer, compression, storage_options)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;124;03m4    4    9\u001B[39;00m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    178\u001B[0m excs_to_catch \u001B[38;5;241m=\u001B[39m (\u001B[38;5;167;01mAttributeError\u001B[39;00m, \u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m)\n\u001B[1;32m--> 179\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n\u001B[0;32m    180\u001B[0m     filepath_or_buffer,\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    182\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m    183\u001B[0m     is_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    184\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    185\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;66;03m# 1) try standard library Pickle\u001B[39;00m\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001B[39;00m\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001B[39;00m\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    191\u001B[0m         \u001B[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001B[39;00m\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:868\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    860\u001B[0m             handle,\n\u001B[0;32m    861\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    864\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    865\u001B[0m         )\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n\u001B[0;32m    869\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    871\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/final/pages_dataframe_final.pkl'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from nltk import word_tokenize\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pywikibot\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from netwulf import visualize\n",
    "import community as community_louvain\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings for presentation sake (still bad practice...)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('danish')) | set(stopwords.words('english'))\n",
    "\n",
    "# Load data\n",
    "pages_dataframe = pd.read_pickle('data/final/pages_dataframe_final.pkl')\n",
    "bots_dataframe = pd.read_pickle('data/final/bots_dataframe_final')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T13:08:59.849629800Z",
     "start_time": "2024-05-08T13:08:50.637888600Z"
    }
   },
   "id": "6b63d2710d72adb8",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Basic stats\n",
    "\n",
    "All the data in this project has been collected by ourselves, using the library 'pywikibot' which provides an easy way to connect to the Wikimedia API. By iterating through all the pages in the Danish Wikipedia, we were able to collect a dataset of roughly 260,000 pages (estimated 85% of all Danish pages), along with all the bots having contributed to those pages, and the categories the pages belong to. All bots on Wikipedia are registered, and by cross-checking the list of bots on the Danish Wikipedia with the contributors to a given page, we could be certain not to get any human users in our dataset.\n",
    "\n",
    "In addition to the page data, we have also gathered information on all the bots, how many pages they have edited, and the total amount of edits they have performed. In all, we collected data on 197 bots (83% of all the bots). Most of the bots not included in this dataset had no contributions to any pages as of yet, and were therefore deemed not to be of much interest. Iterating through all pages is quite time-consuming, since API calls can't be grouped, and various server-side errors meant we had to collect the data in stints. This is the reason why we didn't get a 'complete' dataset, but to the best of our knowledge the collected data is more than representative for the Danish Wikipedia as a whole."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "546fd5f1a8115016"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Graph construction\n",
    "\n",
    "From the page data we can construct a graph with the contributing bots constituting the nodes, with links between them if they have contributed to the same page. Each edge will also contain the amount of pages the two bots both have contributed to together."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12717bdf42b55456"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Getting edges generated for all pages\n",
    "edge_dict = defaultdict(lambda: {'pages':[],'n_pages':0})\n",
    "only_contributor = defaultdict(lambda: 0)\n",
    "pbar = tqdm(total=pages_dataframe.shape[0])\n",
    "def page_edges(page):\n",
    "    pbar.update(1)\n",
    "    contributors = page['bots']\n",
    "    if len(contributors) < 2:\n",
    "        if len(contributors) == 1:\n",
    "            only_contributor[contributors[0]] += 1\n",
    "        return []\n",
    "    contributors.sort()\n",
    "    contributor_pairs = list(combinations(contributors, r=2))\n",
    "    for contributor_pair in contributor_pairs:\n",
    "        edge_dict[contributor_pair]['pages'].append(page['name'])\n",
    "        edge_dict[contributor_pair]['n_pages'] += 1\n",
    "\n",
    "# Constructing the graph from the edge dictionary\n",
    "pages_dataframe.apply(page_edges, axis=1)\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edge_dict.keys())\n",
    "# Remembering to set the attributes, so that we can use the number of pages in an edge later\n",
    "nx.set_edge_attributes(G, edge_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.771834Z"
    }
   },
   "id": "f5cc5da1c44a5c08",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the construction of the graph, we took note of those pages where only one bot have contributed to, and now we can add the number of pages contributed to solely by a certain bot to our bot data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4363195012028e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add new data to the bots dataframe\n",
    "only_contributor = pd.DataFrame(list(only_contributor.items()), columns=['name', 'only contributor count'])\n",
    "bots_dataframe = pd.merge(bots_dataframe, only_contributor, on='name', how='outer').fillna({'only contributor count':0})\n",
    "bots_dataframe['degree'] = bots_dataframe['name'].apply(lambda name: dict(G.degree)[name])\n",
    "\n",
    "# Set node attributes\n",
    "attribute_dict = bots_dataframe.set_index('name').T.to_dict('dict')\n",
    "nx.set_node_attributes(G, attribute_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.776349600Z"
    }
   },
   "id": "5f166e0e7c3fd57c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now visualize the graph. Given the highly connected nature of the graph, normal methods with automatic placement of the nodes doesn't work, so we make our own, placing the nodes on an expanding spiral, in order of degree:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a84a507dfe186aee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "points = np.arange(len(G.nodes))\n",
    "degree_dict = dict(G.degree)\n",
    "radians, radii = points ** 0.8, points ** 0.5\n",
    "coordinates = np.vstack((np.sin(radians), np.cos(radians))).T * np.expand_dims(radii, 1)\n",
    "sorted_nodes = sorted(degree_dict, key=lambda k: degree_dict[k], reverse=True)\n",
    "pos = {sorted_nodes[i]: coordinates[i] for i in range(len(sorted_nodes))}\n",
    "nx.draw(G, node_color='blue', pos=pos,  node_size=10, width=0.02)\n",
    "plt.savefig('graph_visual', dpi=500)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.779358500Z"
    }
   },
   "id": "d8d803c566ede728",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, its like one big cotton ball of connections. Let's try to analyze it. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25eebafa392c9a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Tools, theory and analysis.\n",
    "_Talk about how you’ve worked with text, including regular expressions, unicode, etc.\n",
    "Describe which network science tools and data analysis strategies you’ve used, how those network science measures work, and why the tools you’ve chosen are right for the problem you’re solving.\n",
    "How did you use the tools to understand your dataset?_\n",
    "\n",
    "Now we get to the actual analysis of the data. To summarize: we first analyze the graph, and take a look at some summary stats, the degree distribution, assortativity, and the closeness centralities of the nodes. We then use the Louvain algorithm to detect communities in the graph, which leads us into the textual analysis. Having extracted a good partition from the graph, we try to make sense of the communities using both the categories of pages in them, and the contributions made by the bots. Lastly, guided by the top 10 tokens from the contributions of each bot, we look at a few interesting individuals, to get a sense of the breadth of functions served by the bots.  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e4f2c1b047711fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Graph analysis\n",
    "\n",
    "We first take a look at the summary statistics of the graph:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56de47dd5b66359e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Summary statistics of graph\n",
    "n_nodes = len(G.nodes)\n",
    "n_edges = len(G.edges)\n",
    "mean_degree = np.mean(list(dict(G.degree).values()))\n",
    "n_components = nx.number_connected_components(G)\n",
    "print(f'Number of nodes in graph: {n_nodes}')\n",
    "print(f'Number of edges in graph: {n_edges} out of {n_nodes*(n_nodes-1)/2} possible')\n",
    "print(f'Mean degree <k> in graph: {mean_degree}')\n",
    "print(f'Number of connected components in graph: {n_components}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.783359400Z"
    }
   },
   "id": "2ec9bbba14cb4c32",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the basic stats, we see that the network is extremely connected, since approximately 75% of the possible existing links actually are there. We see that the average degree, 152, is not that far off from the number of nodes, suggesting that many of the bots have contributed to a page with almost all other bots at some point in time. The mean degree is way past log(N), and the graph is therefore in the 'connected' regime, as according to Barabasi [1]."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68c55472a9f3ac75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now take a look at the degree distribution, as contrasted with a random graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b770b7f973180fe0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generating a random graph to compare with\n",
    "p = 2*n_edges/(n_nodes*(n_nodes-1))\n",
    "G_random = nx.gnp_random_graph(n_nodes, p)\n",
    "g_deg_random = dict(G_random.degree).values()\n",
    "\n",
    "# Plotting\n",
    "g_deg = dict(G.degree).values()\n",
    "plt.hist(g_deg, alpha=0.8,bins=30, label='Real graph')\n",
    "plt.hist(g_deg_random, alpha=0.8, label='Random graph')\n",
    "plt.title('Degree distribution')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.savefig('degree_distribution', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.787359300Z"
    }
   },
   "id": "85d3d8042550c0fa",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that, like many other graphs in the real world the degree distribution is far more heavy-tailed than for the random graph. Interestingly, there are many more heavily connected nodes than sparsely connected ones, that is, the distribution shows a heavy right-skew. This is in contrast with many real-world networks where most nodes have quite low degree, and only a few nodes have very high degree."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cd4db565264ad4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now take a look at the distribution of page edit counts for each bot, shown on a log scale, since the edit counts have a large span from 1 to ~200,000. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aa916afd0189ddb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "counts = bots_dataframe['count']\n",
    "plt.hist(bots_dataframe['count'], bins=np.logspace(np.log10(min(counts)), np.log10(max(counts)), 20))\n",
    "plt.xscale('log')\n",
    "plt.title('Histogram of page edit counts (log scale)')\n",
    "plt.xlabel('Page edit count')\n",
    "plt.savefig('edit_distribution', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.792359500Z"
    }
   },
   "id": "389faffcdaf817c5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remembering that the x-axis is on a log scale we see a very left-skewed distribution - in contrast to the right-skewed degree distribution - meaning that most bots have made a low to moderate amount of edits. This is quite interesting, now we take a closer look at the relationship between the degree of a node and the different contribution metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5539b1a8dfd75e90"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.scatter(bots_dataframe['degree'], bots_dataframe['edit_count'], label='edit count')\n",
    "plt.scatter(bots_dataframe['degree'], bots_dataframe['count'], label='page contributions')\n",
    "plt.scatter(bots_dataframe['degree'], bots_dataframe['only contributor count'], label='only contributor')\n",
    "plt.yscale('log')\n",
    "plt.title('Relationship between node degree and contribution metrics')\n",
    "plt.xlabel('Degree')\n",
    "plt.savefig('edit_over_degree', dpi=300)\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.796358900Z"
    }
   },
   "id": "c9d9cc1a81eeb841",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that there is a clear power-law relationship between the degree and the amount of contributions of all types. We thus see that the more bots a given bot has collaborated with, the higher that bots edit count is likely to be, and the effect is more pronounced the higher the degree.\n",
    " \n",
    "The fact that many bots have high degree, but most bots only have a low to moderate amount of edits logically means that there must be a few pages that a significant fraction of all bots have contributed to. Otherwise the striking combination of a right-skew degree distribution and a left-skew edit contribution would not be possible. We take a look to find out if this is indeed the case:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6b3b38979c4e30f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Getting the top pages by number of contributors, and seeing how many unique bots are \n",
    "# among their contributors\n",
    "unique_bots = set()\n",
    "top_10_by_n_contribs = pages_dataframe.sort_values(by='n_contributors', ascending=False).head(10)\n",
    "top_10_by_n_contribs['bots'].head(5).apply(lambda bots: [unique_bots.add(bot) for bot in bots])\n",
    "print(f'Bots having contributed to top 5 pages: {len(unique_bots)} out of {len(bots_dataframe)}')\n",
    "top_10_by_n_contribs[['name', 'n_contributors']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.801594800Z"
    }
   },
   "id": "f1f9a50676adf01c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that there indeed some pages that a third of all bots have contributed to. In addition, we see that the set of contributors to the top 5 pages includes over half of all the bots. This explains how most bots have collaborated with most other bots even though most bots only have contributed to a few pages; a few important pages have made collaborators of the majority of the bots, regardless of edit counts. It comes as no surprise that these pages are on topics of global significance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5972d1b199aad7cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we look at the assortativities of the graph. The assortativity is a way to measure to what degree nodes with similar attributes tend to be linked. A negative score for a given attribute shows a tendency for nodes to be linked with other nodes different from themselves, and vice versa with a positive score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45cf1c13590e9a8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "g_deg_assort = nx.degree_assortativity_coefficient(G)\n",
    "g_count_assort = nx.attribute_assortativity_coefficient(G, 'count')\n",
    "g_edit_count_assort = nx.attribute_assortativity_coefficient(G, 'edit_count')\n",
    "g_only_contrib_assort = nx.attribute_assortativity_coefficient(G, 'only contributor count')\n",
    "\n",
    "print(f'Assortativity by degree: {g_deg_assort}')\n",
    "print(f'Assortativity by page count: {g_count_assort}')\n",
    "print(f'Assortativity by edit count: {g_edit_count_assort}')\n",
    "print(f'Assortativity by only contributor pages: {g_only_contrib_assort}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.808586100Z"
    }
   },
   "id": "ef1e062ea58288bc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the network has too many edges to allow us to feasible compare it to random graphs using the configuration model (with 150,000 edges, each time takes upwards of 6 hours), it is a bit difficult to conclude anything definitively, however, the assortativity by degree and only contributor pages looks like they could be significantly different from random. This would suggest that bots do not tend to collaborate mostly with bots of similar degree, which makes sense in a highly connected network, where most nodes are connected to most other nodes. It would also seem to make sense, unsurprisingly, that bots contributing to many pages as the only bot don't tend to collaborate with other bots that do the same."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e72e9eb67f3ffb09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we look at the closeness centrality. The closeness centrality is a way to express the distance from one node to the other nodes in the network. The scores run from 0 to 1, the higher the score the shorter the average path from the node to the other nodes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b2e5d8db4dc20f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get closeness centrality using nx built-in function.\n",
    "c_centrality = nx.closeness_centrality(G)\n",
    "c_centrality_sorted = dict(sorted(c_centrality.items(), key=lambda item: item[1]))\n",
    "top5_c = list(c_centrality_sorted.items())[:-6:-1]\n",
    "\n",
    "# Print mean and top 5 closeness centralities\n",
    "print(f'Mean centrality: {np.mean(list(c_centrality.values()))}\\n')\n",
    "for name, centrality in top5_c:\n",
    "    print(f'Name: {name}, centrality: {centrality}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.814569300Z"
    }
   },
   "id": "96d4b9c1168a1575",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unsurprisingly - given the highly connected network - we see that the closeness centrality for the top 5 nodes are very high, with the highest actually being 1.0. This is because it is actually connected to all other nodes, which means all bots have collaborated on a page with this bot. We found that this bot, 'Steenthbot', is actually being used to supervise the status of other bots, and edits their pages when their bot status is changed. This explains why it has interacted with all the other bots.\n",
    "\n",
    "We now try to partition the graph and use some textual analysis to get further insights on the network. We now use the Louvain algorithm to attempt to separate the graph into communities for analysis. The Louvain algorithm works by a heuristic, greedy optimization of the modality of different partitions (the modality is a score expressing the density of links within as opposed to outside of the communities defined by a given partition, the higher the 'better'). It should thus give us a sensible partition to work with."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca68337ff93725a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get most likely partition\n",
    "partition = community_louvain.best_partition(G, weight='n_pages')\n",
    "def communities_of_page(bots):\n",
    "    communities = []\n",
    "    for bot in bots:\n",
    "        try:\n",
    "            communities.append(partition[bot])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return list(set(communities))\n",
    "\n",
    "n_communities = max(partition.values()) + 1\n",
    "bots_dataframe['community'] = bots_dataframe['name'].apply(lambda name: partition[name])\n",
    "pages_dataframe['communities'] = pages_dataframe['bots'].apply(communities_of_page)\n",
    "print(f'Number of communities: {n_communities}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.819305800Z"
    }
   },
   "id": "1090aeb95a5a81ab",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the algorithm detects only two communities. This hardly surprising, given the highly connected nature of the network. However, this also means that there is little hope that we can gain much information by analyzing these two communities, but we will try it out. We first use the visualization from above to show the graph with the nodes marked by community:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84cd08ccf61a1cf5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "color_code = {0: 'blue', 1: 'red'}\n",
    "colors = [color_code[partition[node]] for node in G.nodes]\n",
    "nx.draw(G, node_color=colors, pos=pos,  node_size=15, width=0.02)\n",
    "plt.savefig('graph_visual_community', dpi=500)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.826305400Z"
    }
   },
   "id": "dfffa84809e91b1c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is still a bit difficult to decipher, so we look at relative degree distributions for the two communities:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52032aaa03e3f6d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "degree_0 = [degree_dict[node] for node in G.nodes if partition[node] == 0]\n",
    "degree_1 = [degree_dict[node] for node in G.nodes if partition[node] == 1]\n",
    "plt.hist(degree_0, color='blue',bins=20, alpha=0.8, density=True, label='Community 0')\n",
    "plt.hist(degree_1, color='red',bins=20, alpha=0.8, density=True, label='Community 1')\n",
    "plt.legend()\n",
    "plt.title('Degree distribution by community (density)')\n",
    "plt.savefig('degree_community', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.828203200Z"
    }
   },
   "id": "c7ddf1061f7f870f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we see something interesting. It looks like the degree distribution of community 1 is less skewed to the right, and has a lot more nodes with very low degree. We now turn to the content of the edits to gather more information\n",
    "\n",
    "We start by tokenizing the lists of categories each page belongs to, after which we create the corpus for pages belonging to each community. In order to keep the tokens informative we filter out stop words, and a few words found in preliminary analysis to be too common to be informative (in this case 'wikipedia' and 'wikidata'). We also exclude special characters (though not, importantly, non-latin characters)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce157b90cda1f2f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to tokenize category lists.\n",
    "def tokenize_category_list(category_list, exclude_stop=True):\n",
    "    # Adding stop words based on observations of superflous terms\n",
    "    stop_words_category = stop_words | {'wikipedia','wikidata'}\n",
    "    text = ' '.join(category_list)\n",
    "    text = text.lower()\n",
    "    # Filtering out numbers and other characters\n",
    "    text = ''.join([char for char in text if (char.isalpha() or char in ' -')])\n",
    "    tokens = word_tokenize(text)\n",
    "    # Exclude stop words\n",
    "    if exclude_stop:\n",
    "        return [token for token in tokens if not token in stop_words_category]\n",
    "    return tokens\n",
    "\n",
    "pages_dataframe['tokens'] = pages_dataframe['categories'].apply(tokenize_category_list)\n",
    "# Generating the corpus based on communities\n",
    "corpus = pages_dataframe[['communities','tokens']].explode('communities').groupby('communities')['tokens'].apply(lambda x: list(chain(*x)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.831166200Z"
    }
   },
   "id": "b3f79e3fcc8afb34",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now define the functions needed to perform TF-IDF analysis on the tokens. TF-IDF allows us to find informative tokens by finding frequent terms and weighing them by the information content of the term across the corpus."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ae4999055c33d41"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Returns top k terms as defined by TF-IDF\n",
    "def top_k_terms_TF_IDF(tokens, k, IDF_dict):\n",
    "    tokens, counts = np.unique(tokens, return_counts=True)\n",
    "    tfidf = np.zeros(len(counts))\n",
    "    for i, (token, count) in enumerate(list(zip(tokens, counts))):\n",
    "        tfidf[i] = count * IDF_dict[token]\n",
    "    top_k_idxs = np.argsort(tfidf)[:-(k+1):-1]\n",
    "    return tokens[top_k_idxs]\n",
    "\n",
    "# Returns top k most frequent terms.\n",
    "def top_k_terms_TF(tokens, k):\n",
    "    unique_tokens, counts = np.unique(tokens, return_counts=True)\n",
    "    top_k_idxs = np.argsort(counts)[:-(k+1):-1]\n",
    "    return np.array(unique_tokens)[top_k_idxs]\n",
    "\n",
    "# For the whole corpus define the IDF value for each term and return\n",
    "# the results as a dictionary: {term:IDF}\n",
    "def IDF(corpus):\n",
    "    IDF_dict = defaultdict(lambda: 0)\n",
    "    N = len(corpus)\n",
    "    for doc in corpus:\n",
    "        for term in list(set(doc)):\n",
    "            IDF_dict[term] += 1\n",
    "    return {term: np.log(N / count) for term, count in IDF_dict.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.834165600Z"
    }
   },
   "id": "96a9d6e50de8b1c2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "IDF_dict = IDF(corpus)\n",
    "TFIDF_df = pd.DataFrame(data={c: top_k_terms_TF_IDF(corpus[c], 10, IDF_dict) for c in range(2)})\n",
    "TFIDF_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.835165400Z"
    }
   },
   "id": "bb1260163a53dbe7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that this data is a bit difficult to get any meaningful information out of. It would seem like the second community has made a lot of edits on films, although this can also be due to a few very active bots. The top 10 tokens of the second community shows no real pattern. Instead of looking at the pages for the different communities, we can try taking a look at the bots themselves, and the text in their contributions.\n",
    "\n",
    "We first tokenize the contributions:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30df7750eee5526f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize_contributions(contributions, exclude_stop=True):\n",
    "    # Adding superflous/uninformatively common words to stop words set.\n",
    "    stop_words_contributions = stop_words | {'robot','r','tilføjer','ændrer','fjerner', 'enwpawbawb'}\n",
    "    text = ' '.join(contributions)\n",
    "    text = text.lower()\n",
    "    # Filter out numbers and symbols\n",
    "    text = ''.join([char for char in text if (char.isalpha() or char in ' -')])\n",
    "    tokens = word_tokenize(text)\n",
    "    # Exclude stop words.\n",
    "    if exclude_stop:\n",
    "        return [token for token in tokens if not token in stop_words_contributions]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.837361800Z"
    }
   },
   "id": "b1dfa45d4f66eb9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We calculate the 10 most frequent tokens for each bot, and collect the corpus for each community."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64773579ae1cefc1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pbar = tqdm(total=bots_dataframe.shape[0])\n",
    "def top_k_tokens(bot_name, k=10):\n",
    "    pbar.update(1)\n",
    "    site = pywikibot.Site('da', 'wikipedia')\n",
    "    bot = pywikibot.User(site, bot_name)\n",
    "    contributions = list(bot.contributions())\n",
    "    contributions_text = [c[-1] for c in contributions]\n",
    "    tokens = tokenize_contributions(contributions_text)\n",
    "    return top_k_terms_TF(tokens, k)\n",
    "\n",
    "bots_dataframe['top 10 tokens'] = bots_dataframe['name'].apply(top_k_tokens)\n",
    "corpus = bots_dataframe.groupby('community')['top 10 tokens'].apply(lambda x: list(chain(*x)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.839641600Z"
    }
   },
   "id": "7fceea8ebd0ebd37",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now look at the TF-IDF terms:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2e11bbc6fc9db95"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "IDF_dict = IDF(corpus)\n",
    "TFIDF_df = pd.DataFrame(data={c: top_k_terms_TF_IDF(corpus[c], 10, IDF_dict) for c in range(2)})\n",
    "TFIDF_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.844705500Z"
    }
   },
   "id": "7be164d45ab27f38",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we see a clearer pattern. It looks like the first community has many bots maintaining the interlinking structure of the Danish wikipedia to wikis in other languages (ar, ga, lt, ml and br are language codes for Arabic, Irish, Lithuanian, Malaysian, and Breton), whereas the second community shows more activity related to updating other types of data for the pages. Otherwise there is not that much to go on, and further down we discuss what - if given more time - could have been tried to get more information out of the partition.\n",
    "\n",
    "Looking at the top 10 tokens for the bots, we identified a few of them, that exhibit the breadth of functions served by the bots."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7acfe1038f5eeee3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define function to get contributions from API based on username\n",
    "def get_user_contributions(username):\n",
    "    site = pywikibot.Site('da', 'wikipedia')\n",
    "    user = pywikibot.User(site, username)\n",
    "    contributions = user.contributions(total=5)\n",
    "    return [contrib[3] for contrib in list(contributions)]\n",
    "\n",
    "# Get and display data from the chosen example bots\n",
    "example_bots = ['Steenthbot', 'NivlekDaBot', 'DanKoehlBot', 'Rubinbot']\n",
    "for bot_name in example_bots:\n",
    "    print(f'Name of bot: {bot_name}\\n', '-'*20)\n",
    "    bot_data = bots_dataframe[bots_dataframe['name'] == bot_name]\n",
    "    count, edit_count, community, top_10, degree = bot_data['count'], bot_data['edit_count'], bot_data['community'], bot_data['top 10 tokens'], bot_data['degree']\n",
    "    print(f'Contributions count: {int(count)}, edit count: {int(edit_count)}, community: {int(community)}, degree: {int(degree)}\\n')\n",
    "    print(f'Top 10 tokens: {list(top_10)[0]} \\n')\n",
    "    last_five = get_user_contributions(bot_name)[:5]\n",
    "    print('Last five edits:','\\n'.join(last_five), '\\n')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T13:08:59.847580900Z"
    }
   },
   "id": "f98fa55e0571e4ea",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we see some of the different functions that bots serve on the Danish wikipedia:\n",
    "\n",
    "'Rubinbot' is clearly mostly occupied with maintaining interoperability between the Danish Wikipedia and other language wikipedias, by linking similar/identical pages in the two languages.\n",
    " \n",
    "'Steenthbot' is the most active bot on the whole Danish wikipedia with contributions to over 200,000 pages, and from the top 10 tokens we see that most of its work has to do with updating backend data, and 'cosmetic' changes.\n",
    "\n",
    " 'NivlekDaBot' seems to be engaged in editing data on sports pages, notably on handball and football.\n",
    " \n",
    " Lastly, 'DanKoehlBot' only has five edits in total, all of them about correcting a common Danish spelling mistake of 'idag' instead of 'i dag', meaning 'yesterday'.\n",
    " \n",
    "We see that the first three bots are all quite highly connected with degrees over 100, despite two of them having fairly moderate edit counts. The last bot 'DanKoehlBot' only has 5 edits, but still a degree of 25. From these examples we get a sense of the variety of contributions by bots on the Danish wikipedia, and see clearly that it would be unrealistic to expect humans to make these fairly tedious edits, especially if relying on volunteers, as is the case for Wikipedia. \n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e40565270399c737"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summarization of findings\n",
    "\n",
    " We see that the network of bots is highly connected, meaning that most bots have collaborated with most other bots at some point in time, which is also reflected by the very high mean closeness centrality of 0.84. The degree distribution is skewed to the right, with many highly connected bots, whereas the edit count distribution is skewed to the right, with most bots having a low to moderate amount of edits. We saw that this caused by a few pages, that many, many bots contribute to - pages of high importance like those on countries, religions, and global organizations. This leads to a community that is challenging to partition neatly, even though some tendencies can be seen where some bots are more focused on providing cross-references to wikis of other languages.\n",
    "  \n",
    "From our textual analysis we were able to gain insights into the various functions served by the bots ranging from fixing typos, to updating the way information is presented on sports pages, linking the Danish wikipedia to the Persian wiki, to meta-data on bot activity. In making these tedious but important edits, the bots are clearly essential in ensuring that Wikipedia remain one of the top go-to places for information on the internet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "793aec5bfea9fabe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Discussion\n",
    "\n",
    "In collecting the dataset we succesfully created a quite large dataset, which enabled us to get a comprehensive picture of the way bots contribute to the wiki. Through our network analysis we saw how the network of collaborating bots was very highly connected. The fact that the degree distribution is so heavily right-skewed is quite interesting, and in contrast to many human networks, were most people are not especially connected, with only a few highly connected individuals - like the network of collaborating scientists analyzed earlier in the course. It would be interesting to further investigate under which circumstances networks tend to have right- or left-skewed degree distributions.\n",
    "\n",
    "In this project we had expected the analysis of the communities and their most frequent terms to be more informative than they were. The reason that wasn't the case was firstly the highly connected nature of the network, which led to Louvain algorithm having difficulties in neatly separating the network into communities. Secondly, due to the sparse and specialized nature of the contributions text, making the tokens a bit difficult to parse in a consistent way, although we were still able to gain some insights into the contributions of different bots.\n",
    "\n",
    "If we were to improve on the analysis we would probably try to find some more ways to use textual analysis to shed light on the activities of the different bots. One way could be to somehow define some categorizations for the different edits, and compare the frequencies with which the different categories appear across communities. We could also try to draw text from the pages themselves. The reason we did not do this is that most of the edits performed by the bots seemed not so much to relate to the content of the pages themselves, but rather the meta-data and structure of the pages, which arguably our analysis confirmed. Still, there might be insights to draw in choosing to focus more on the content of the pages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e81d3f1e08e4bc63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
